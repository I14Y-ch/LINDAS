name: Weekly Concept2Lindas Update

on:
  schedule:
    - cron: '0 0 * * 1'  # 00:00 every Monday
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.10'
  STARDOG_URL: 'https://stardog-test.cluster.ldbar.ch/lindas'
  TARGET_GRAPH: 'https://lindas.admin.ch/fso/i14y'
  BATCH_SIZE: '30'  # strings in env; cast to int in code

jobs:
  get-concepts:
    runs-on: ubuntu-latest
    outputs:
      concept_ids: ${{ steps.get-concepts.outputs.concept_ids }}
      total_concepts: ${{ steps.get-concepts.outputs.total_concepts }}
      batch_matrix: ${{ steps.get-concepts.outputs.batch_matrix }}
      output_file: ${{ steps.get-output-filename.outputs.filename }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install rdflib requests

      - name: Get OUTPUT_FILE_NAME from Python config
        id: get-output-filename
        run: |
          OUTPUT_FILE=$(python -c "from concept2sharedDimension.src.versioning.config import OUTPUT_FILE_NAME; print(OUTPUT_FILE_NAME)")
          echo "filename=$OUTPUT_FILE" >> $GITHUB_OUTPUT

      - name: Get all concept IDs + build matrix
        id: get-concepts
        shell: bash
        run: |
          python - <<'PY'
          import json, os
          from concept2sharedDimension.src.versioning.utils import get_all_concepts
          from concept2sharedDimension.src.versioning.config import STATUSES

          concepts = get_all_concepts(STATUSES)
          ids = [c['id'] for c in concepts]
          print(f"Total concepts found: {len(ids)}")

          batch_size = int(os.environ.get('BATCH_SIZE','30'))
          n_batches = (len(ids) + batch_size - 1) // batch_size
          print(f"Will create {n_batches} batches of size {batch_size}")
          
          # Run at least one empty batch so downstream jobs execute cleanly.
          matrix = {"include": [{"batch_index": i} for i in range(max(1, n_batches))]}

          with open(os.environ['GITHUB_OUTPUT'], 'a') as gh:
            print(f"concept_ids={','.join(ids)}", file=gh)
            print(f"total_concepts={len(ids)}", file=gh)
            print(f"batch_matrix={json.dumps(matrix)}", file=gh)
          PY

  process-batches:
    needs: get-concepts
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix: ${{ fromJSON(needs.get-concepts.outputs.batch_matrix) }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install rdflib requests

      - name: Calculate batch range
        id: calc-batch
        shell: bash
        run: |
          CONCEPT_IDS='${{ needs.get-concepts.outputs.concept_ids }}'
          BATCH_SIZE=${{ env.BATCH_SIZE }}
          BATCH_INDEX=${{ matrix.batch_index }}

          # Convert comma-separated string to array
          IFS=',' read -ra ID_ARRAY <<< "$CONCEPT_IDS"
          TOTAL_IDS=${#ID_ARRAY[@]}

          echo "Total concept IDs: $TOTAL_IDS"
          echo "Batch size: $BATCH_SIZE" 
          echo "Batch index: $BATCH_INDEX"

          START=$((BATCH_INDEX * BATCH_SIZE))
          END=$((START + BATCH_SIZE))

          echo "Calculated range: $START to $END"

          # Handle case where END exceeds array size
          if [ $END -gt $TOTAL_IDS ]; then
            END=$TOTAL_IDS
          fi

          BATCH_IDS=""
          for ((i=START; i<END && i<TOTAL_IDS; i++)); do
            if [ -n "$BATCH_IDS" ]; then
              BATCH_IDS="$BATCH_IDS,${ID_ARRAY[i]}"
            else
              BATCH_IDS="${ID_ARRAY[i]}"
            fi
          done

          echo "Batch IDs for batch $BATCH_INDEX: $BATCH_IDS"
          echo "Processing concepts from index $START to $((END-1))"

          {
            echo "batch_ids=$BATCH_IDS"
            echo "batch_start=$START"
            echo "batch_end=$END"
            echo "actual_batch_size=$(echo "$BATCH_IDS" | tr ',' '\n' | wc -l)"
          } >> "$GITHUB_OUTPUT"

      - name: Process batch ${{ matrix.batch_index }} (${{ steps.calc-batch.outputs.batch_start }}-${{ steps.calc-batch.outputs.batch_end }})
        if: steps.calc-batch.outputs.batch_ids != ''
        env:
          BATCH_CONCEPT_IDS: ${{ steps.calc-batch.outputs.batch_ids }}
        shell: bash
        run: |
          echo "Processing batch ${{ matrix.batch_index }}"
          echo "Concept IDs: $BATCH_CONCEPT_IDS"
          echo "Batch size: ${{ steps.calc-batch.outputs.actual_batch_size }}"
          
          python - <<'PY'
          import os
          from concept2sharedDimension.src.main import main
          from concept2sharedDimension.src.versioning.config import CONCEPT_IDS

          batch_ids_str = os.environ.get('BATCH_CONCEPT_IDS', '')
          if batch_ids_str.strip():
              batch_ids = [x.strip() for x in batch_ids_str.split(',') if x.strip()]
              if batch_ids:
                  print(f"Processing batch of {len(batch_ids)} concepts: {batch_ids[:3]}...")
                  # Clear the config list and add our batch
                  CONCEPT_IDS.clear()
                  CONCEPT_IDS.extend(batch_ids)
                  main()
              else:
                  print("No valid concept IDs in batch")
          else:
              print("Empty batch - skipping")
          PY

      - name: Upload batch artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: batch-${{ matrix.batch_index }}-output
          path: |
            ${{ needs.get-concepts.outputs.output_file }}
            batch_${{ matrix.batch_index }}_${{ needs.get-concepts.outputs.output_file }}
          if-no-files-found: warn

  combine-and-upload:
    needs: [get-concepts, process-batches]
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
  
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install rdflib requests
          
      - name: Download all batch artifacts
        uses: actions/download-artifact@v4
        with:
          path: batches
          pattern: batch-*-output
          merge-multiple: true

      - name: List downloaded files
        run: |
          echo "Downloaded batch files:"
          find batches -name "*.ttl" -type f | sort
          echo "File sizes:"
          find batches -name "*.ttl" -type f -exec ls -lh {} \;

      - name: Combine RDF files
        shell: bash
        run: |
          shopt -s nullglob
          files=(batches/*.ttl)
          
          echo "Found ${#files[@]} TTL files to combine"
          
          if [ ${#files[@]} -eq 0 ]; then
            echo "No TTL files found; creating empty final_output.ttl"
            echo "# Empty file - no concepts processed" > final_output.ttl
          else
            echo "Combining files: ${files[*]}"
            cat "${files[@]}" > combined_output.ttl
            
            # Check if combined file has content
            if [ -s combined_output.ttl ]; then
              echo "Combined file size: $(wc -l < combined_output.ttl) lines"
              
              # Deduplicate prefixes and clean up
              python - <<'PY'
          import re
          src = 'combined_output.ttl'
          dst = 'final_output.ttl'
          
          with open(src, 'r', encoding='utf-8') as f:
              content = f.read()
          
          prefixes = {}
          other_lines = []
          
          for line in content.splitlines():
              line = line.strip()
              if line.startswith('@prefix'):
                  # Extract prefix name
                  m = re.match(r'@prefix\s+(\S+):', line)
                  if m:
                      prefix_name = m.group(1)
                      if prefix_name not in prefixes:
                          prefixes[prefix_name] = line
                  else:
                      # If regex fails, just add it
                      prefixes[len(prefixes)] = line
              elif line:  # Skip empty lines during processing
                  other_lines.append(line)
          
          # Write output with prefixes first, then content
          with open(dst, 'w', encoding='utf-8') as f:
              # Write all unique prefixes
              for prefix_line in prefixes.values():
                  f.write(prefix_line + '\n')
              f.write('\n')  # Empty line after prefixes
              
              # Write content
              f.write('\n'.join(other_lines))
              if other_lines:  # Add final newline if there's content
                  f.write('\n')
          PY
            else
              echo "Combined file is empty"
              echo "# No content generated" > final_output.ttl
            fi
          fi

      - name: Validate combined RDF
        shell: bash
        run: |
          if [ -s final_output.ttl ]; then
            echo "Validating RDF file..."
            python - <<'PY'
          from rdflib import Graph
          import sys
          
          try:
              g = Graph()
              g.parse('final_output.ttl', format='turtle')
              print(f'Validated combined RDF with {len(g)} triples')
              
              # Count concepts for verification
              from rdflib.namespace import RDF
              from rdflib import Namespace
              SDO = Namespace("http://schema.org/")
              vl = Namespace("https://version.link/#")
              
              identities = len(list(g.subjects(RDF.type, SDO.DefinedTermSet)))
              versions = len(list(g.subjects(RDF.type, vl.Version)))
              
              print(f'Found {identities} total DefinedTermSet objects')
              print(f'Found {versions} version objects')
              
          except Exception as e:
              print(f'RDF validation failed: {e}')
              sys.exit(1)
          PY
          else
            echo "final_output.ttl is empty; skipping validation."
          fi

      - name: Clear Stardog Graph
        env:
          STARDOG_USER: ${{ secrets.STARDOG_USER }}
          STARDOG_PASSWORD: ${{ secrets.STARDOG_PASSWORD_TEST }}
        shell: bash
        run: |
          python - <<'PY'
          import os, warnings, requests
          warnings.filterwarnings("ignore")
          auth = requests.auth.HTTPBasicAuth(os.environ['STARDOG_USER'], os.environ['STARDOG_PASSWORD'])
          url = os.environ['STARDOG_URL'].rstrip('/') + '/update'
          target = os.environ['TARGET_GRAPH']
          r = requests.post(
              url,
              headers={'Content-Type': 'application/sparql-update'},
              auth=auth,
              data=f'CLEAR GRAPH <{target}>',
              verify=False
          )
          r.raise_for_status()
          print('Graph cleared successfully')
          PY

      - name: Upload to Stardog
        env:
          STARDOG_USER: ${{ secrets.STARDOG_USER }}
          STARDOG_PASSWORD: ${{ secrets.STARDOG_PASSWORD_TEST }}
        shell: bash
        run: |
          if [ -s final_output.ttl ]; then
            echo "Uploading to Stardog..."
            python - <<'PY'
          import os, warnings, requests
          warnings.filterwarnings("ignore")
          auth = requests.auth.HTTPBasicAuth(os.environ['STARDOG_USER'], os.environ['STARDOG_PASSWORD'])
          url = os.environ['STARDOG_URL']
          params = {'graph': os.environ['TARGET_GRAPH']}
          
          with open('final_output.ttl', 'rb') as f:
            r = requests.post(
                url, 
                params=params, 
                headers={'Content-Type': 'text/turtle'}, 
                auth=auth, 
                data=f, 
                verify=False
            )
            r.raise_for_status()
            print('Upload successful')
          PY
          else
            echo "No data to upload - final_output.ttl is empty"
          fi  

      - name: Archive final results
        uses: actions/upload-artifact@v4
        with:
          name: final-codelist-catalog
          path: final_output.ttl
          if-no-files-found: error
